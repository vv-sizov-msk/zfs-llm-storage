# README.md
# zfs-llm-storage — ZFS-хранилище для LLM/AI

## Обзор

Этот проект разворачивает ZFS-хранилище для задач LLM/AI (inference, training, VM, backup) на базе:

- HP ProLiant MicroServer Gen10
- 4× HDD HGST 4 ТБ
- NVMe Intel P4610 1.92 ТБ (SLOG + L2ARC)
- NVMe Intel P4510 960 ГБ (special vdev + L2ARC)
- Linux (рекомендуется Ubuntu 24.04 LTS + OpenZFS 2.2.x)

Пул экспонируется по:

- **NFSv4** — для моделей, датасетов, backup
- **iSCSI (LIO)** — для VM/гостевых файловых систем

Цель — максимизировать **низкую латентность и IOPS** под смешанную нагрузку (LLM + VM), сохраняя отказоустойчивость и предсказуемое поведение при resilver.

---

## Архитектура ZFS

### Схема пула

Пул `llm`:

- **vdev’ы данных:**
  - `mirror(HGST0, HGST1)`
  - `mirror(HGST2, HGST3)`
- **special vdev:**
  - Intel P4510 (раздел p1, ~400 ГБ)  
    `special_small_blocks=32K` — метаданные + блоки ≤ 32K на NVMe.
- **SLOG (log vdev):**
  - Intel P4610 (раздел p1, 32 ГБ)  
    Ускоряет sync-записи NFS/iSCSI/VM.
- **L2ARC (cache vdev):**
  - остальная часть P4610 (p2)
  - остальная часть P4510 (p2)
- **ARC в RAM:** ~11 ГБ (на хосте с 16 ГБ RAM и ролью “storage-first”).

### Почему mirror, а не RAIDZ

1. **Случайные I/O и латентность**

   - LLM/VM активно бьют по диску мелкими и средними блоками с рандомным доступом.
   - Mirror даёт:
     - в среднем ~2× IOPS на чтение на каждый mirror vdev (чтение с любого из двух дисков),
     - более простую и предсказуемую латентность.
   - RAIDZ (особенно с малым числом дисков) хуже по латентности на случайных I/O, т.к. требует чтения/записи целых полос с вычислением чётности.

2. **Resilver и отказоустойчивость при нагрузке**

   - Mirror resilver’ится **блок-ориентированно** (только реально используемые блоки), что:
     - быстрее,
     - меньше нагружает пул,
     - снижает риск “второй смерти” диска под нагрузкой.
   - RAIDZ resilver’ит полосы — под высокой нагрузкой LLM/VM это дольше и тяжелее.

3. **Масштабируемость**

   - Добавив ещё один mirror vdev из пары дисков, вы линейно увеличиваете:
     - полезную ёмкость,
     - aggregate IOPS.
   - Для домашних/мелких стораджей с постепенными апгрейдами это удобнее, чем переразбираться с RAIDZ-топологией.

4. **Когда RAIDZ лучше**

   RAIDZ всё ещё отличный выбор для:

   - “холодного” архива,
   - медиатеки,
   - больших последовательных файлов,
   - бэкапов, где важна ёмкость/стоимость, а не мгновенная латентность.

   В этом проекте такими задачами занимается датасет `backup`, но базовая топология всё равно нацелена на online-нагрузки.

### special vdev на P4510

- Метаданные ZFS и блоки ≤ 32K живут на **быстром NVMe**.
- Эффект:
  - сильно снижается латентность операций с метаданными (листинг, открытие большого числа файлов, random-read мелочи),
  - уменьшается фрагментация HDD.
- Порог `special_small_blocks=32K`:
  - не даёт special vdev “забетонироваться” крупными блоками,
  - концентрирует на нём именно то, что лучше всего выигрывает от NVMe.

### SLOG на P4610 (раздел p1, 32 ГБ)

- Используется для sync-операций (`sync=standard`/`sync=always`).
- Критичен для:
  - NFS с `sync` (запись чекпоинтов, метаданных),
  - iSCSI-записей от VM (гостевые FS ожидают корректной семантики sync).
- 32 ГБ почти всегда достаточно; увеличение размера SLOG редко даёт ощутимый профит.

> Важно: потеря **SLOG-устройства** обычно приводит к потере последних синхронных транзакций, но **не разрушает пул**.

### L2ARC на P4610(p2) + P4510(p2)

- Расширяет эффективный кэш чтения сверх ARC в RAM.
- Полезен, когда:
  - рабочий набор данных/моделей заметно больше 11 ГБ,
  - есть повторяющиеся паттерны чтения (частые инференсы тех же моделей).

### Разметка SSD

- P4610:
  - `p1` “SLOG” — 32 GiB под SLOG,
  - `p2` “L2ARC” — остальное под L2ARC.
- P4510:
  - `p1` “SPECIAL” — ~400 GiB под special vdev,
  - `p2` “L2ARC” — остаток под L2ARC.

---

## Логическое деление данных

Используются отдельные датасеты:

- `llm/inference`
  - `recordsize=1M`
  - потоки чтения моделей, индексных структур.
- `llm/training`
  - `recordsize=1M`, `logbias=throughput`
  - длинные записи чекпоинтов, батчи данных.
- `llm/tmp`
  - `recordsize=128K`, `sync=disabled` (осознанно, для временных данных)
  - scratch-каталоги, кеши, промежуточные артефакты.
- `llm/backup`
  - `recordsize=1M`, `compression=lz4`
  - архивы моделей, snapshot-dump’ы, экспортные дампы.
- `llm/vm` + `llm/vm/iscsi0` (zvol)
  - `volblocksize=16K`, `sync=always`, `volmode=dev`
  - блок-устройство под iSCSI для гипервизора/хоста VM.

---

## Стратегии работы с таким хранилищем

### 1. Общие рекомендации

- Держать заполнение пула **≤ 80%**, лучше ~70% — ZFS сильно деградирует по латентности на почти заполненном пуле.
- Регулярно делать:
  - `zpool scrub llm` (раз в месяц),
  - проверять `zpool status` после scrub и по cron/systemd-timer’у.
- Следить за состоянием special vdev:
  - `zfs list -t filesystem -o name,usedbysnapshots,usedbydataset,logicalused`,
  - `zpool list -v` — для оценки заполнения special.

### 2. Inference

- Модели/чекпоинты:
  - Хранить на `llm/inference` и экспортировать по NFS.
  - На клиентах монтировать с опциями типа: `noatime,nolock,vers=4.1`.
- Если один из клиентов — хост с локальной GPU для LLM, можно:
  - держать горячую модель на локальном NVMe,
  - использовать ZFS-пул как backend для хранения всех версий/бэкапов.

### 3. Training

- Рабочие датасеты (которые часто переписываются) — в `llm/training`.
- Тяжёлые временные файлы, промежуточные артефакты — в `llm/tmp`:
  - `sync=disabled` даёт заметный прирост производительности.
  - **Обязательно**: это только для данных, которые не критичны при падении питания.
- Регулярно выгружать итоговые модели/чекпоинты в `llm/backup` (snapshot + send/recv или rsync/archive).

### 4. VM через iSCSI

- `llm/vm/iscsi0` — zvol, отданный по iSCSI:
  - подключается на хосте-гипервизоре (Proxmox/ESXi/другой Linux),
  - на нём создаётся гостевая ФС (ext4/xfs) или LVM с VM-дисками.
- Рекомендуется:
  - включить CHAP, если сегмент не полностью доверенный,
  - ограничить доступ по IP/initiator IQN,
  - не делить один zvol между сильно разными нагрузками (лучше несколько zvol под разные группы VM).

### 5. Backup-стратегия

- Основные механизмы:

  - **ZFS snapshot’ы** по расписанию:
    - `llm/inference@daily-YYYYMMDD`,
    - `llm/training@hourly-YYYYMMDD-HH`,
    - `llm/vm@daily-YYYMMDD`.
  - **`zfs send | zfs recv`** на отдельный backup-хост/диск.
  - Плюс внешняя копия самых важных моделей/чекпоинтов (off-site/облако).

- Базовый подход:
  - “рабочие” snapshot’ы — малые retention (1–7 дней),
  - “архивные” snapshot’ы на backup-пуле — месяцы и годы.

---

## Лучшие практики эксплуатации

1. **Мониторинг**

   - Prometheus + node_exporter (textfile-collector) + Grafana:
     - health пула (`zfs_zpool_health`),
     - iostat (read/write bytes),
     - ARC (size, hit ratio),
     - L2ARC (size, hit ratio),
     - latency по NFS/iSCSI (через сторонние экспортеры/трейсинг).

2. **Тюнинг под нагрузку**

   - ARC:
     - 11 ГБ на 16 ГБ RAM оправдано, если узел почти не крутит прикладные сервисы.
     - Если на нём же живут VM/контейнеры — уменьшить ARC до 6–8 ГБ.
   - L2ARC:
     - не стоит забивать все NVMe исключительно под L2ARC;
     - special vdev приоритетнее для случайных I/O и метаданных.
   - `recordsize`:
     - 1M там, где файлы большие и читаются/пишутся целиком (модели, чекпоинты),
     - меньший блок (16K/32K/128K) там, где случайный доступ.

3. **Обслуживание**

   - Раз в месяц:
     - `zpool scrub llm`,
     - проверка логов SMART/дисков,
     - проверка заполнения special vdev и пула в целом.
   - Раз в квартал:
     - ревизия snapshot-политики (не копится ли мусор),
     - ревизия iSCSI/NFS ACL/Firewall.

---

## Замечания по безопасности

- Любые операции `parted`, `zpool create/add`, `targetcli create` **разрушительны** при неправильных устройствах → всегда сверяй `lsblk` и by-id.
- `sync=disabled` на `llm/tmp` — сознательный компромисс: ускорение ценой риска потери последних записей при сбое питания.
- ARC=11 ГБ на 16 ГБ RAM — допустимо, если узел в первую очередь **storage-сервер**; если на нём же живут VM/контейнеры — ARC обязательно уменьшать.
- iSCSI без CHAP и с порталом `0.0.0.0` нормально только в **строго изолированном** сторедж-сегменте.

---

### Противоречия

- Задача одновременно быть и “маленьким NAS”, и backend’ом под LLM/VM. В некоторых сценариях захочется **отдельный** RAIDZ-пул под холодный архив — это другая топология.

### Низкая уверенность

- Точные объёмы и профили нагрузки (сколько TB моделей/данных, сколько VM) → под них можно более тонко подстраивать `recordsize`, `special_small_blocks` и размер L2ARC.

### Что уточнить

- Планируется ли на этом же HP крутить ещё и hypervisor/контейнеры, или он 100% сторедж?
- Есть ли отдельный backup-узел ZFS (второй пул) или backup будет делаться на этот же сервер/облако?

### Верификация

- Описанная схема ZFS и скрипт корректны для HP MicroServer Gen10 с 4×HGST 4ТБ + 2×NVMe и ориентированы на LLM/VM-нагрузку.
- Ограничения: базируется на Linux (Debian/Ubuntu), использует LIO/targetcli, UFW, node_exporter; для других дистрибутивов (RHEL/Alma) нужны небольшие адаптации пакетных имён и сервисов.
